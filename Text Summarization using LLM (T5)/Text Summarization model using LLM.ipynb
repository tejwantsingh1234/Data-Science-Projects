{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0fff90d-a787-4bbd-bb65-41f085101579",
   "metadata": {},
   "source": [
    "# Text Summarization Model using LLM\n",
    "\n",
    "A text summarization model takes a long text and creates a shorter version while preserving its main points. It works by extracting key sentences directly (extractive summarization) or rephrasing the content into a shorter form (abstractive summarization). \n",
    "\n",
    "To build a text summarization model, first, we need to choose a pre-trained language model like T5. Then, we need to tokenize the input text, which converts it into a format the model can process. The next step will be to use the model to generate a summary by specifying parameters like max length and beam search for better results. The final step will be to decode the generated tokens back into readable text and adjust parameters to improve the summary quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5d6b02-0c39-4cae-bce0-d74256d838f3",
   "metadata": {},
   "source": [
    "## Selecting a suitable LLM\n",
    "\n",
    "The T5 model (Text-to-Text Transformer) by Google is one such model that is effective for various text-based tasks like translation, question-answering, and summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d45ca-5798-4496-88a1-eb0beef41dad",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb65117-b4cb-4505-9b36-965aded7f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da80b3d1-bbf6-426e-8566-5c3a868bf832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef674677c474d9398f736971ffc9c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Sehaj\\anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tejwa\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f31011a9f584e2bac4ecbecf6a918c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2e0de6124947628fa4effd035bc977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d598e8b-58cd-4794-a950-e361f0987c25",
   "metadata": {},
   "source": [
    "To select between t5-small, t5-base, or t5-large, consider your computational resources and accuracy needs. t5-small is faster and requires less memory, which makes it suitable for quick tasks or limited hardware. t5-base offers a balance between speed and performance, ideal for general use. t5-large provides the highest accuracy but needs more memory and processing power, which makes it better for scenarios where performance is more important than speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf0171b-361a-4d1f-bf02-e4553bda923c",
   "metadata": {},
   "source": [
    "## Preparing the text for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a0d6365-dff9-4b93-a7ed-ea696e116960",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Gaming is an engaging form of entertainment that combines creativity, strategy, and skill. It offers immersive experiences through diverse genres like action, adventure, and simulation.Gaming fosters teamwork, problem-solving, and reflex development, making it both fun and enriching. Whether casual or competitive, gaming connects players and sparks imagination worldwide. \n",
    "\"\"\"\n",
    "input_text = \"summarize: \" + text\n",
    "# the prefix \"summarize: \" for the T5 model is necessary because T5 is a \"text-to-text\" model that needs to understand what task it should perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b4945-c719-4506-9414-5e1d492d51fd",
   "metadata": {},
   "source": [
    "## Tokenizing the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed0b6755-31c0-4083-bcc7-09f8ef300c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(input_text,\n",
    "                            return_tensors = 'pt', # Pytorch tensor\n",
    "                            max_length = 512,\n",
    "                            truncation = True) # if exceeeds the max_length, it will be truncated to fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace448e2-dc74-4e27-a288-1fe4bd9f2c33",
   "metadata": {},
   "source": [
    "## Generating the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "071c6321-3250-4fa3-9230-ed96420d4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length = 50,\n",
    "    num_beams = 4, \n",
    "# beam search is a decoding strategy that keeps the top num_beams most likely sequences at each step, rather than just the most likely sequence\n",
    "    length_penalty = 2.0, # prevents excessively long outputs\n",
    "    early_stopping = True # Stops the generation process as soon as all beams generate an end-of-sequence token.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc10886-18fa-4f4e-8ab4-8375e7d54732",
   "metadata": {},
   "source": [
    "## Decoding and displaying the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a28f39d-997b-4a1e-9b98-1f827159fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:  gaming is an engaging form of entertainment that combines creativity, strategy, and skill. it fosters teamwork, problem-solving, and reflex development.\n"
     ]
    }
   ],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens = True)\n",
    "print('Summary: ', summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
